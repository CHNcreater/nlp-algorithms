{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"我喜欢吃苹果\",\n",
    "          \"我喜欢吃香蕉\",\n",
    "          \"她喜欢吃葡萄\",\n",
    "          \"他不喜欢吃香蕉\",\n",
    "          \"他喜欢吃苹果\",\n",
    "          \"她喜欢吃草莓\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我', '喜', '欢', '吃', '苹', '果']\n",
      "['我', '喜', '欢', '吃', '香', '蕉']\n",
      "['她', '喜', '欢', '吃', '葡', '萄']\n",
      "['他', '不', '喜', '欢', '吃', '香', '蕉']\n",
      "['他', '喜', '欢', '吃', '苹', '果']\n",
      "['她', '喜', '欢', '吃', '草', '莓']\n"
     ]
    }
   ],
   "source": [
    "def tokenizer(text):\n",
    "    return [char for char in text]\n",
    "\n",
    "for c in corpus:\n",
    "    print(tokenizer(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('我',) Counter({'喜': 2})\n",
      "('喜',) Counter({'欢': 6})\n",
      "('欢',) Counter({'吃': 6})\n",
      "('吃',) Counter({'苹': 2, '香': 2, '葡': 1, '草': 1})\n",
      "('苹',) Counter({'果': 2})\n",
      "('香',) Counter({'蕉': 2})\n",
      "('她',) Counter({'喜': 2})\n",
      "('葡',) Counter({'萄': 1})\n",
      "('他',) Counter({'不': 1, '喜': 1})\n",
      "('不',) Counter({'喜': 1})\n",
      "('草',) Counter({'莓': 1})\n"
     ]
    }
   ],
   "source": [
    "#计算词频\n",
    "from collections import defaultdict, Counter\n",
    "def count_ngrams(corpus, n):\n",
    "    counts = defaultdict(Counter)\n",
    "    for text in corpus:\n",
    "        tokens = tokenizer(text)\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            ngram = tuple(tokens[i:i+n])\n",
    "            prefix = ngram[:-1]\n",
    "            token = ngram[-1]\n",
    "            counts[prefix][token] += 1\n",
    "    return counts\n",
    "\n",
    "bigram_counts = count_ngrams(corpus, 2)\n",
    "for prefix, counter in bigram_counts.items():\n",
    "    print(prefix, counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('我',) {'喜': 1.0}\n",
      "('喜',) {'欢': 1.0}\n",
      "('欢',) {'吃': 1.0}\n",
      "('吃',) {'苹': 0.3333333333333333, '香': 0.3333333333333333, '葡': 0.16666666666666666, '草': 0.16666666666666666}\n",
      "('苹',) {'果': 1.0}\n",
      "('香',) {'蕉': 1.0}\n",
      "('她',) {'喜': 1.0}\n",
      "('葡',) {'萄': 1.0}\n",
      "('他',) {'不': 0.5, '喜': 0.5}\n",
      "('不',) {'喜': 1.0}\n",
      "('草',) {'莓': 1.0}\n"
     ]
    }
   ],
   "source": [
    "#计算概率\n",
    "def ngrams_probability(ngrams_counts):\n",
    "    probs = defaultdict(dict)\n",
    "    for prefix, counter in ngrams_counts.items():\n",
    "        total = sum(counter.values())\n",
    "        for token, count in counter.items():\n",
    "            probs[prefix][token] = count / total\n",
    "    return probs\n",
    "bigram_probability = ngrams_probability(bigram_counts)\n",
    "for prefix, prob in bigram_probability.items():\n",
    "    print(prefix, prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我不喜欢吃苹果\n"
     ]
    }
   ],
   "source": [
    "def generate_text_token(prefix, ngrams_probability):\n",
    "    if prefix not in ngrams_probability:\n",
    "        return \"\"\n",
    "    next_token_probs = ngrams_probability[prefix]\n",
    "    next_token = max(next_token_probs, key=next_token_probs.get)\n",
    "    return next_token\n",
    "\n",
    "def generate_text(prefix, ngrams_probability, n, max_length=10):\n",
    "    text = list(prefix)\n",
    "    for _ in range(max_length-len(prefix)):\n",
    "        token = generate_text_token(tuple(text[-(n-1):]), ngrams_probability)\n",
    "        if not token:\n",
    "            break\n",
    "        text.append(token)\n",
    "    return \"\".join(text)\n",
    "\n",
    "generated_text = generate_text(\"我不喜欢\", bigram_probability, 2)\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
